{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLWmQf9SWrN0ScNNTPFpZF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYYQN5FLSFBd"
      },
      "outputs": [],
      "source": [
        "# Installing necessary dependencies\n",
        "try:\n",
        "    import pyspark\n",
        "except ModuleNotFoundError:\n",
        "    !pip install pyspark\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "except ModuleNotFoundError:\n",
        "    !pip install tensorflow\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except ModuleNotFoundError:\n",
        "    !pip install imbalanced-learn\n",
        "\n",
        "try:\n",
        "    from flask import Flask, request, jsonify\n",
        "except ModuleNotFoundError:\n",
        "    !pip install flask\n",
        "\n",
        "!pip install keras\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Capstone Project\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "wRSHDfvVSRl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Initial Data Exploration\n",
        "def initial_data_exploration(data_path):\n",
        "    df = spark.read.csv(data_path, header=True, inferSchema=True, sep=\",\", multiLine=True)\n",
        "    df = df.filter(df['Fecha'].isNotNull())\n",
        "    df = df.select('Fecha', 'TCR 1/').withColumnRenamed('TCR 1/', 'TCR_1')\n",
        "    return df"
      ],
      "metadata": {
        "id": "p2S0PGQVSV9m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Extract, Transform, Load (ETL)\n",
        "def etl_process(df):\n",
        "    df_clean = df.dropna()\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "Dp8UXJHlSgfF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Feature Creation\n",
        "def feature_creation(df):\n",
        "    pd_df = df.toPandas()\n",
        "    pd_df['Fecha'] = pd.to_datetime(pd_df['Fecha'], format='%d/%m/%Y')\n",
        "\n",
        "    # Scale TCR_1\n",
        "    tcr_scaler = StandardScaler()\n",
        "    pd_df['TCR_1_scaled'] = tcr_scaler.fit_transform(pd_df[['TCR_1']])\n",
        "\n",
        "    # Create lag features\n",
        "    for lag in range(1, 8):\n",
        "        pd_df[f'TCR_1_lag_{lag}'] = pd_df['TCR_1_scaled'].shift(lag)\n",
        "\n",
        "    pd_df.dropna(inplace=True)\n",
        "\n",
        "    # Scale lag features\n",
        "    numerical_cols = [f'TCR_1_lag_{lag}' for lag in range(1, 8)]\n",
        "    scaler = StandardScaler()\n",
        "    pd_df[numerical_cols] = scaler.fit_transform(pd_df[numerical_cols])\n",
        "\n",
        "    return pd_df, scaler, tcr_scaler"
      ],
      "metadata": {
        "id": "OqqsLqeNSlVc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Model Definition\n",
        "def define_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_shape=(input_shape,), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "x9Qit0o3SmIa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Model Training\n",
        "def train_model(model, X_train, y_train, X_val, y_val, epochs=20, batch_size=32):\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)\n",
        "    return history"
      ],
      "metadata": {
        "id": "Soojw_KySqEh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Model Evaluation\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Mean Squared Error: {mse}\")"
      ],
      "metadata": {
        "id": "8d4_ZCCJSvFX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Predicting Next Week's TCR_1\n",
        "def predict_next_week(model, last_week_data, scaler, feature_columns, tcr_scaler):\n",
        "    predictions = []\n",
        "    for _ in range(7):\n",
        "        # Ensure last_week_data has the correct shape (2D array)\n",
        "        last_week_df = pd.DataFrame(last_week_data, columns=feature_columns)\n",
        "        scaled_data = scaler.transform(last_week_df)\n",
        "        next_prediction = model.predict(scaled_data)\n",
        "\n",
        "        # Descale the prediction\n",
        "        next_prediction_descaled = tcr_scaler.inverse_transform(next_prediction)\n",
        "        predictions.append(next_prediction_descaled[0, 0])\n",
        "\n",
        "        # Create new input by shifting last week data and adding the new prediction\n",
        "        last_week_data = np.roll(last_week_data, -1)\n",
        "        last_week_data[-1] = next_prediction[0, 0]\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "ROByeoQiSxRj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the data\n",
        "def plot_data(processed_data, next_week_predictions, next_week_dates):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot all original data\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(processed_data['Fecha'], processed_data['TCR_1'], label='Original Data')\n",
        "    plt.title('All Original Data')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('TCR_1')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot last week's data\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(processed_data['Fecha'].tail(7), processed_data['TCR_1'].tail(7), label='Last Week')\n",
        "    plt.title('Last Week Data')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('TCR_1')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot next week's predictions\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(next_week_dates, next_week_predictions, label='Predictions', color='orange')\n",
        "    plt.title('Next Week Predictions')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('TCR_1')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot all data including predictions\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(processed_data['Fecha'], processed_data['TCR_1'], label='Original Data')\n",
        "    plt.plot(next_week_dates, next_week_predictions, label='Predictions', color='orange')\n",
        "    plt.title('All Data Including Predictions')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('TCR_1')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot scatter data\n",
        "def plot_scatter_data(processed_data, next_week_predictions, next_week_dates):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Scatter plot of all data including predictions\n",
        "    plt.scatter(processed_data['Fecha'], processed_data['TCR_1'], label='Original Data', alpha=0.6)\n",
        "    plt.scatter(next_week_dates, next_week_predictions, label='Predictions', color='orange', alpha=0.6)\n",
        "    plt.title('All Data Including Predictions (Scatter Plot)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('TCR_1')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "76Gxl6FBS2AH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    data_path = \"/content/historico_rango.csv\"\n",
        "\n",
        "    df = initial_data_exploration(data_path)\n",
        "    df_clean = etl_process(df)\n",
        "    processed_data, scaler, tcr_scaler = feature_creation(df_clean)\n",
        "\n",
        "    # Debugging: Print processed_data to check the columns and data\n",
        "    print(\"Processed Data:\")\n",
        "    print(processed_data.tail(10))  # Print the last 10 rows for inspection\n",
        "\n",
        "    target_column = 'TCR_1_scaled'\n",
        "    feature_columns = [col for col in processed_data.columns if col.startswith('TCR_1_lag')]\n",
        "\n",
        "    X = processed_data[feature_columns]\n",
        "    y = processed_data[target_column]\n",
        "\n",
        "    print(f\"Shapes of X and y: {X.shape}, {y.shape}\")\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"Shapes of X_train, X_val, y_train, y_val: {X_train.shape}, {X_val.shape}, {y_train.shape}, {y_val.shape}\")\n",
        "\n",
        "    input_shape = X_train.shape[1]\n",
        "    model = define_model(input_shape)\n",
        "    train_model(model, X_train, y_train, X_val, y_val)\n",
        "    evaluate_model(model, X_val, y_val)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(\"model.h5\")\n",
        "    print(\"Model saved as model.h5\")\n",
        "\n",
        "    # Predicting the next week's TCR_1\n",
        "    last_week_data = processed_data[feature_columns].tail(7).values\n",
        "\n",
        "    # Ensure last_week_data is a 2D array\n",
        "    if last_week_data.shape[0] != 7:\n",
        "        last_week_data = np.tile(last_week_data, (7, 1))\n",
        "\n",
        "    print(f\"Shape of last_week_data before prediction: {last_week_data.shape}\")\n",
        "\n",
        "    next_week_predictions = predict_next_week(model, last_week_data, scaler, feature_columns, tcr_scaler)\n",
        "    next_week_dates = pd.date_range(start=processed_data['Fecha'].max() + pd.Timedelta(days=1), periods=7)\n",
        "\n",
        "    # Displaying the prediction\n",
        "    print(\"Next week's predictions:\")\n",
        "    for date, pred in zip(next_week_dates, next_week_predictions):\n",
        "        print(f\"Date: {date.date()}, Predicted TCR_1: {pred}\")\n",
        "\n",
        "    # Plotting the data\n",
        "    plot_data(processed_data, next_week_predictions, next_week_dates)\n",
        "\n",
        "    # Plotting scatter data\n",
        "    plot_scatter_data(processed_data, next_week_predictions, next_week_dates)"
      ],
      "metadata": {
        "id": "RiSVR_-DS5Ca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}